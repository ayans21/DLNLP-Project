{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "# import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg.eigen.arpack import eigsh\n",
    "import sys\n",
    "import re\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "#from utils.utils import clean_str, loadWord2Vec  \n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import torch\n",
    "\n",
    "sys.path.append('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda install -c dglteam dgl-cuda10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['20ng', 'r8', 'R52', 'ohsumed', 'mr']\n",
    "# build corpus\n",
    "dataset = datasets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_docs = []\n",
    "train_data = []\n",
    "test_data = []\n",
    "train_docs = []\n",
    "test_docs = []\n",
    "label=[]\n",
    "train_label=[]\n",
    "test_label=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./data/' + dataset + '.train.txt', 'r', errors='ignore') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        train_data.append(line.strip())\n",
    "        temp = line.split(\"\\t\")\n",
    "        train_label.append(temp[0])\n",
    "        label.append(temp[0])\n",
    "        train_docs.append(temp[1])\n",
    "        total_docs.append(temp[1])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('./data/' + dataset + '.test.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        test_data.append(line.strip())\n",
    "        temp = line.split(\"\\t\")\n",
    "        test_label.append(temp[0])\n",
    "        label.append(temp[0])\n",
    "        test_docs.append(temp[1])\n",
    "        total_docs.append(temp[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}  # to remove rare words\n",
    "\n",
    "for line in total_docs:\n",
    "    temp = clean_str(line)\n",
    "    words = temp.split()\n",
    "    for word in words:\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_doc = []\n",
    "for line in total_docs:\n",
    "    temp = clean_str(line)\n",
    "    words = temp.split()\n",
    "    doc_words = []\n",
    "    for word in words:\n",
    "        # word not in stop_words and word_freq[word] >= 5\n",
    "        if dataset == 'mr':\n",
    "            doc_words.append(word)\n",
    "        elif word not in stop_words and word_freq[word] >= 5:\n",
    "            doc_words.append(word)\n",
    "\n",
    "    doc_str = ' '.join(doc_words).strip()\n",
    "    clean_doc.append(doc_str)\n",
    "    \n",
    "clean_corpus_str = '\\n'.join(clean_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data/' + dataset + '.clean.txt', 'w') as f:\n",
    "    f.write(clean_corpus_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_len = 10000\n",
    "aver_len = 0\n",
    "max_len = 0 \n",
    "with open('./data/' + dataset + '.clean.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        temp = line.split()\n",
    "        aver_len = aver_len + len(temp)\n",
    "        if len(temp) < min_len:\n",
    "            min_len = len(temp)\n",
    "        if len(temp) > max_len:\n",
    "            max_len = len(temp)\n",
    "\n",
    "aver_len = 1.0 * aver_len / len(lines)\n",
    "print('Min_len : ' + str(min_len))\n",
    "print('Max_len : ' + str(max_len))\n",
    "print('Average_len : ' + str(aver_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ids = list(range(len(train_label)))\n",
    "test_ids = list(range(len(train_ids),len(label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = {}\n",
    "word_set = set()\n",
    "with open('./data/' + dataset + '.clean.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        temp = line.split()\n",
    "        for word in temp:\n",
    "            word_set.add(word)\n",
    "            if word in word_freq:\n",
    "                word_freq[word] += 1\n",
    "            else:\n",
    "                word_freq[word] = 1\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_dictionary(dict): \n",
    "  \n",
    "    # __init__ function \n",
    "    def __init__(self): \n",
    "        self = dict() \n",
    "          \n",
    "    # Function to add key:value \n",
    "    def add(self, key, value): \n",
    "        self[key] = value \n",
    "  \n",
    "# Main Function \n",
    "word_ids = my_dictionary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(vocab)):\n",
    "    word_ids.add(i, vocab[i])     \n",
    "len(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=len(train_ids)+len(word_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_doc_word=np.zeros([s,s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_doc_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_corpus_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "with open('./data/' + dataset + '.clean.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        temp = line.split()\n",
    "        doc_str = ' '.join(temp).strip()\n",
    "        corpus.append(doc_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_word_tfidf=X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word co-occurence with context windows\n",
    "window_size = 20\n",
    "windows = []\n",
    "\n",
    "for doc_words in corpus:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        # print(length, length - window_size + 1)\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "            # print(window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_id_map = {}\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pair_count = {}\n",
    "for window in windows:\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = word_id_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = word_id_map[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pair_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "# pmi as weights\n",
    "\n",
    "num_window = len(windows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtowords_pmi=np.identity(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordtowords_pmi.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "    pmi = log((1.0 * count / num_window) /\n",
    "              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    wordtowords_pmi[i][j]=pmi\n",
    "    wordtowords_pmi[j][i]=pmi\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_weight = np.concatenate((doc_word_tfidf, wordtowords_pmi), axis=0)\n",
    "edge_mat= np.concatenate((doc_word_tfidf, wordtowords_pmi), axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_mat[edge_mat > 0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_nodes=edge_mat.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs=len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge=[]\n",
    "p=[]\n",
    "for i in range(tot_nodes):\n",
    "    for j in range(vocab_size):\n",
    "        if(edge_mat[i][j]==1):\n",
    "            p.append(i)\n",
    "            p.append(j+num_docs)\n",
    "            edge.append(p)\n",
    "            p=[]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding self edge\n",
    "for i in range(num_docs):\n",
    "    p=[]\n",
    "    p.append(i)\n",
    "    p.append(i)\n",
    "    edge.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(edge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=np.identity(tot_nodes)\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_idx=[]\n",
    "dst_idx=[]\n",
    "for i in range(len(edge)):\n",
    "    src_idx.append(edge[i][0])\n",
    "    dst_idx.append(edge[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = np.asarray(src_idx)\n",
    "dst = np.asarray(dst_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dgl.DGLGraph((src,dst))\n",
    "print(g.node_attr_schemes())\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(vocab_size):\n",
    "    label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(label)==tot_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=np.zeros(tot_nodes)\n",
    "for i in train_ids:\n",
    "    s[i]=1\n",
    "for i in range(7674,15017):\n",
    "    s[i]=1\n",
    "train_mask=(s>0)\n",
    "train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=np.zeros(tot_nodes)\n",
    "for i in test_ids:\n",
    "    s[i]=1\n",
    "for i in range(7674,15017):\n",
    "    s[i]=1\n",
    "test_mask=(s>0)\n",
    "test_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(label)\n",
    "print(integer_encoded)\n",
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "output = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "#output = onehot_encoder.fit_transform(integer_encoded)\n",
    "#print(onehot_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out=len(np.unique(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features= torch.from_numpy(features)\n",
    "g_feat=features\n",
    "g_feat=g_feat.to(device)\n",
    "g_feat=g_feat.type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(g_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output= torch.from_numpy(output)\n",
    "output=output.to(device)\n",
    "output=output.type(torch.LongTensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=output.reshape([tot_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embed_feature_size is the output dim of first layer\n",
    "embed_feature_size=16\n",
    "fesize=g_feat.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_matrix=np.zeros(35)\n",
    "acc_test_matrix=np.zeros(35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jumping Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "import torch\n",
    "\n",
    "AGGREGATIONS = {\n",
    "    'sum': torch.sum,\n",
    "    'mean': torch.mean,\n",
    "    'max': torch.max,\n",
    "}\n",
    "\n",
    "\n",
    "class GraphConvLayer(torch.nn.Module):\n",
    "    \"\"\"Graph convolution layer.\n",
    "\n",
    "    Args:\n",
    "        in_features (int): Size of each input node.\n",
    "        out_features (int): Size of each output node.\n",
    "        aggregation (str): 'sum', 'mean' or 'max'.\n",
    "                           Specify the way to aggregate the neighbourhoods.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, aggregation='sum'):\n",
    "        super(GraphConvLayer, self).__init__()\n",
    "\n",
    "        if aggregation not in AGGREGATIONS.keys():\n",
    "            raise ValueError(\"'aggregation' argument has to be one of \"\n",
    "                             \"'sum', 'mean' or 'max'.\")\n",
    "        self.aggregate = lambda nodes: AGGREGATIONS[aggregation](nodes, dim=1)\n",
    "\n",
    "        self.linear = torch.nn.Linear(in_features, out_features)\n",
    "        self.self_loop_w = torch.nn.Linear(in_features, out_features)\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(out_features))\n",
    "\n",
    "    def forward(self, graph, x):\n",
    "        graph.ndata['h'] = x\n",
    "        graph.update_all(\n",
    "            fn.copy_src(src='h', out='msg'),\n",
    "            lambda nodes: {'h': self.aggregate(nodes.mailbox['msg'])})\n",
    "        h = graph.ndata.pop('h')\n",
    "        h = self.linear(h)\n",
    "        return h + self.self_loop_w(x) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from .graph_conv_layer import GraphConvLayer\n",
    "\n",
    "\n",
    "class JKNetConcat(torch.nn.Module):\n",
    "    \"\"\"An implementation of Jumping Knowledge Network (arxiv 1806.03536) which\n",
    "    combine layers with concatenation.\n",
    "\n",
    "    Args:\n",
    "        in_features (int): Size of each input node.\n",
    "        out_features (int): Size of each output node.\n",
    "        n_layers (int): Number of the convolution layers.\n",
    "        n_units (int): Size of the middle layers.\n",
    "        aggregation (str): 'sum', 'mean' or 'max'.\n",
    "                           Specify the way to aggregate the neighbourhoods.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, n_layers=6, n_units=16,\n",
    "                 aggregation='sum'):\n",
    "        super(JKNetConcat, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.gconv0 = GraphConvLayer(in_features, n_units, aggregation)\n",
    "        self.dropout0 = torch.nn.Dropout(0.5)\n",
    "        for i in range(1, self.n_layers):\n",
    "            setattr(self, 'gconv{}'.format(i),\n",
    "                    GraphConvLayer(n_units, n_units, aggregation))\n",
    "            setattr(self, 'dropout{}'.format(i), torch.nn.Dropout(0.5))\n",
    "        self.last_linear = torch.nn.Linear(n_layers * n_units, out_features)\n",
    "\n",
    "    def forward(self, graph, x):\n",
    "        layer_outputs = []\n",
    "        for i in range(self.n_layers):\n",
    "            dropout = getattr(self, 'dropout{}'.format(i))\n",
    "            gconv = getattr(self, 'gconv{}'.format(i))\n",
    "            x = dropout(F.relu(gconv(graph, x)))\n",
    "            layer_outputs.append(x)\n",
    "\n",
    "        h = torch.cat(layer_outputs, dim=1)\n",
    "        return self.last_linear(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#from .graph_conv_layer import GraphConvLayer\n",
    "\n",
    "\n",
    "class JKNetMaxpool(torch.nn.Module):\n",
    "    \"\"\"An implementation of Jumping Knowledge Network (arxiv 1806.03536) which\n",
    "    combine layers with Maxpool.\n",
    "\n",
    "    Args:\n",
    "        in_features (int): Size of each input node.\n",
    "        out_features (int): Size of each output node.\n",
    "        n_layers (int): Number of the convolution layers.\n",
    "        n_units (int): Size of the middle layers.\n",
    "        aggregation (str): 'sum', 'mean' or 'max'.\n",
    "                           Specify the way to aggregate the neighbourhoods.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, n_layers=6, n_units=16,\n",
    "                 aggregation='sum'):\n",
    "        super(JKNetMaxpool, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.gconv0 = GraphConvLayer(in_features, n_units, aggregation)\n",
    "        self.dropout0 = torch.nn.Dropout(0.5)\n",
    "        for i in range(1, self.n_layers):\n",
    "            setattr(self, 'gconv{}'.format(i),\n",
    "                    GraphConvLayer(n_units, n_units, aggregation))\n",
    "            setattr(self, 'dropout{}'.format(i), torch.nn.Dropout(0.5))\n",
    "        self.last_linear = torch.nn.Linear(n_units, out_features)\n",
    "\n",
    "    def forward(self, graph, x):\n",
    "        layer_outputs = []\n",
    "        for i in range(self.n_layers):\n",
    "            dropout = getattr(self, 'dropout{}'.format(i))\n",
    "            gconv = getattr(self, 'gconv{}'.format(i))\n",
    "            x = dropout(F.relu(gconv(graph, x)))\n",
    "            layer_outputs.append(x)\n",
    "\n",
    "        h = torch.stack(layer_outputs, dim=0)\n",
    "        h = torch.max(h, dim=0)[0]\n",
    "        return self.last_linear(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  crucial Hyperparameters\n",
    "#n_layers=number of layers in network\n",
    "#n_units=number of units in network\n",
    "#node_aggregation= ['sum','mean','max']\n",
    "n_layers=2\n",
    "n_units=128\n",
    "node_aggregation='sum'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just uncommnet the model you want to use currently to JKNET CONCAT\n",
    "\n",
    "model = JKNetConcat(fesize, out, n_layers,n_units,node_aggregation).to(device)\n",
    "#model = JKNetMaxpool(fesize, out, n_layers,n_units,node_aggregation).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = th.max(logits, dim=1)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_matrix=np.zeros(50)\n",
    "acc_test_matrix=np.zeros(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "optimizer = th.optim.Adam(net.parameters(),  lr=0.03, weight_decay=0.05)\n",
    "dur = []\n",
    "for epoch in range(45):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    net.train()\n",
    "    logits = model(g, g_feat)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], output[train_mask])\n",
    "    Train_acc = evaluate(net, g, g_feat, output, train_mask)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Train Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), Train_acc, np.mean(dur)))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    Test_acc = evaluate(net, g, g_feat, output, test_mask)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), Test_acc, np.mean(dur)))\n",
    "    train_acc=(((torch.from_numpy(np.array(Train_acc, dtype=np.float64)))))\n",
    "    test_acc=(((torch.from_numpy(np.array(Test_acc, dtype=np.float64)))))\n",
    "\n",
    "    acc_train_matrix[epoch]=train_acc\n",
    "    acc_test_matrix[epoch]=test_acc\n",
    "\"\"\"\n",
    "    if(epoch>5 and acc_train_matrix[epoch]-acc_train_matrix[epoch-1]<0.0005 and \n",
    "       acc_test_matrix[epoch]-acc_test_matrix[epoch-1]<0.0005):\n",
    "        break\n",
    "    \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
