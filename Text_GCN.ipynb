{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from sklearn import metrics\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "# import networkx as nx\n",
    "import scipy.sparse as sp\n",
    "from math import log\n",
    "from sklearn import svm\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from utils.utils import *\n",
    "from models.gcn import GCN\n",
    "from models.mlp import MLP\n",
    "\n",
    "from config import CONFIG\n",
    "cfg = CONFIG()\n",
    "\n",
    "\n",
    "if len(sys.argv) != 2:\n",
    "\tsys.exit(\"Use: python train.py <dataset>\")\n",
    "\n",
    "datasets = ['20ng', 'R8', 'R52', 'ohsumed', 'mr']\n",
    "dataset = sys.argv[1]\n",
    "\n",
    "if dataset not in datasets:\n",
    "\tsys.exit(\"wrong dataset name\")\n",
    "cfg.dataset = dataset\n",
    "\n",
    "# Set random seed\n",
    "seed = random.randint(1, 200)\n",
    "seed = 2019\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_index_file(filename):\n",
    "    index = []\n",
    "    for line in open(filename):\n",
    "        index.append(int(line.strip()))\n",
    "    return index\n",
    "\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datasets = ['R8', 'R52', 'ohsumed', 'mr']\n",
    "# build corpus\n",
    "dataset=datasets[1]\n",
    "\n",
    "word_vector_map = {}\n",
    "\n",
    "# shulffing\n",
    "doc_name_list = []\n",
    "doc_train_list = []\n",
    "doc_test_list = []\n",
    "\n",
    "with open('../data/' + dataset + '.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc_name_list.append(line.strip())\n",
    "        temp = line.split(\"\\t\")\n",
    "        if temp[1].find('test') != -1:\n",
    "            doc_test_list.append(line.strip())\n",
    "        elif temp[1].find('train') != -1:\n",
    "            doc_train_list.append(line.strip())\n",
    "\n",
    "\n",
    "doc_content_list = []\n",
    "with open('../data/corpus/' + dataset + '.clean.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        doc_content_list.append(line.strip())\n",
    "# print(doc_content_list)\n",
    "\n",
    "train_ids = []\n",
    "for train_name in doc_train_list:\n",
    "    train_id = doc_name_list.index(train_name)\n",
    "    train_ids.append(train_id)\n",
    "print(train_ids)\n",
    "random.shuffle(train_ids)\n",
    "\n",
    "# partial labeled data\n",
    "#train_ids = train_ids[:int(0.2 * len(train_ids))]\n",
    "\n",
    "train_ids_str = '\\n'.join(str(index) for index in train_ids)\n",
    "with open('../data/' + dataset + '.train.index', 'w') as f:\n",
    "    f.write(train_ids_str)\n",
    "\n",
    "\n",
    "test_ids = []\n",
    "for test_name in doc_test_list:\n",
    "    test_id = doc_name_list.index(test_name)\n",
    "    test_ids.append(test_id)\n",
    "print(test_ids)\n",
    "random.shuffle(test_ids)\n",
    "\n",
    "test_ids_str = '\\n'.join(str(index) for index in test_ids)\n",
    "with open('../data/' + dataset + '.test.index', 'w') as f:\n",
    "    f.write(test_ids_str)\n",
    "\n",
    "\n",
    "ids = train_ids + test_ids\n",
    "print(ids)\n",
    "print(len(ids))\n",
    "\n",
    "shuffle_doc_name_list = []\n",
    "shuffle_doc_words_list = []\n",
    "for id in ids:\n",
    "    shuffle_doc_name_list.append(doc_name_list[int(id)])\n",
    "    shuffle_doc_words_list.append(doc_content_list[int(id)])\n",
    "shuffle_doc_name_str = '\\n'.join(shuffle_doc_name_list)\n",
    "shuffle_doc_words_str = '\\n'.join(shuffle_doc_words_list)\n",
    "\n",
    "\n",
    "with open('../data/corpus/' + dataset + '_shuffle.txt', 'w') as f:\n",
    "    f.write(shuffle_doc_words_str)\n",
    "\n",
    "\n",
    "# build vocab\n",
    "word_freq = {}\n",
    "word_set = set()\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_set.add(word)\n",
    "        if word in word_freq:\n",
    "            word_freq[word] += 1\n",
    "        else:\n",
    "            word_freq[word] = 1\n",
    "\n",
    "vocab = list(word_set)\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "word_doc_list = {}\n",
    "\n",
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    appeared = set()\n",
    "    for word in words:\n",
    "        if word in appeared:\n",
    "            continue\n",
    "        if word in word_doc_list:\n",
    "            doc_list = word_doc_list[word]\n",
    "            doc_list.append(i)\n",
    "            word_doc_list[word] = doc_list\n",
    "        else:\n",
    "            word_doc_list[word] = [i]\n",
    "        appeared.add(word)\n",
    "\n",
    "word_doc_freq = {}\n",
    "for word, doc_list in word_doc_list.items():\n",
    "    word_doc_freq[word] = len(doc_list)\n",
    "\n",
    "word_id_map = {}\n",
    "for i in range(vocab_size):\n",
    "    word_id_map[vocab[i]] = i\n",
    "\n",
    "vocab_str = '\\n'.join(vocab)\n",
    "\n",
    "with open('../data/corpus/' + dataset + '_vocab.txt', 'w') as f:\n",
    "    f.write(vocab_str)\n",
    "\n",
    "\n",
    "# label list\n",
    "label_set = set()\n",
    "for doc_meta in shuffle_doc_name_list:\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label_set.add(temp[2])\n",
    "label_list = list(label_set)\n",
    "\n",
    "label_list_str = '\\n'.join(label_list)\n",
    "with open('../data/corpus/' + dataset + '_labels.txt', 'w') as f:\n",
    "    f.write(label_list_str)\n",
    "\n",
    "\n",
    "# x: feature vectors of training docs, no initial features\n",
    "# slect 90% training set\n",
    "train_size = len(train_ids)\n",
    "val_size = int(0.1 * train_size)\n",
    "real_train_size = train_size - val_size  # - int(0.5 * train_size)\n",
    "# different training rates\n",
    "\n",
    "real_train_doc_names = shuffle_doc_name_list[:real_train_size]\n",
    "real_train_doc_names_str = '\\n'.join(real_train_doc_names)\n",
    "\n",
    "with open('../data/' + dataset + '.real_train.name', 'w') as f:\n",
    "    f.write(real_train_doc_names_str)\n",
    "\n",
    "\n",
    "row_x = []\n",
    "col_x = []\n",
    "data_x = []\n",
    "for i in range(real_train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            # print(doc_vec)\n",
    "            # print(np.array(word_vector))\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_x.append(i)\n",
    "        col_x.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_x.append(doc_vec[j] / doc_len)  # doc_vec[j]/ doc_len\n",
    "\n",
    "# x = sp.csr_matrix((real_train_size, word_embeddings_dim), dtype=np.float32)\n",
    "x = sp.csr_matrix((data_x, (row_x, col_x)), shape=(\n",
    "    real_train_size, word_embeddings_dim))\n",
    "\n",
    "y = []\n",
    "for i in range(real_train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    y.append(one_hot)\n",
    "y = np.array(y)\n",
    "print(y)\n",
    "\n",
    "# tx: feature vectors of test docs, no initial features\n",
    "test_size = len(test_ids)\n",
    "\n",
    "row_tx = []\n",
    "col_tx = []\n",
    "data_tx = []\n",
    "for i in range(test_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i + train_size]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_tx.append(i)\n",
    "        col_tx.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_tx.append(doc_vec[j] / doc_len)  # doc_vec[j] / doc_len\n",
    "\n",
    "# tx = sp.csr_matrix((test_size, word_embeddings_dim), dtype=np.float32)\n",
    "tx = sp.csr_matrix((data_tx, (row_tx, col_tx)),\n",
    "                   shape=(test_size, word_embeddings_dim))\n",
    "\n",
    "ty = []\n",
    "for i in range(test_size):\n",
    "    doc_meta = shuffle_doc_name_list[i + train_size]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ty.append(one_hot)\n",
    "ty = np.array(ty)\n",
    "print(ty)\n",
    "\n",
    "# allx: the the feature vectors of both labeled and unlabeled training instances\n",
    "# (a superset of x)\n",
    "# unlabeled training instances -> words\n",
    "\n",
    "word_vectors = np.random.uniform(-0.01, 0.01,\n",
    "                                 (vocab_size, word_embeddings_dim))\n",
    "\n",
    "for i in range(len(vocab)):\n",
    "    word = vocab[i]\n",
    "    if word in word_vector_map:\n",
    "        vector = word_vector_map[word]\n",
    "        word_vectors[i] = vector\n",
    "\n",
    "row_allx = []\n",
    "col_allx = []\n",
    "data_allx = []\n",
    "\n",
    "for i in range(train_size):\n",
    "    doc_vec = np.array([0.0 for k in range(word_embeddings_dim)])\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_len = len(words)\n",
    "    for word in words:\n",
    "        if word in word_vector_map:\n",
    "            word_vector = word_vector_map[word]\n",
    "            doc_vec = doc_vec + np.array(word_vector)\n",
    "\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i))\n",
    "        col_allx.append(j)\n",
    "        # np.random.uniform(-0.25, 0.25)\n",
    "        data_allx.append(doc_vec[j] / doc_len)  # doc_vec[j]/doc_len\n",
    "for i in range(vocab_size):\n",
    "    for j in range(word_embeddings_dim):\n",
    "        row_allx.append(int(i + train_size))\n",
    "        col_allx.append(j)\n",
    "        data_allx.append(word_vectors.item((i, j)))\n",
    "\n",
    "\n",
    "row_allx = np.array(row_allx)\n",
    "col_allx = np.array(col_allx)\n",
    "data_allx = np.array(data_allx)\n",
    "\n",
    "allx = sp.csr_matrix(\n",
    "    (data_allx, (row_allx, col_allx)), shape=(train_size + vocab_size, word_embeddings_dim))\n",
    "\n",
    "ally = []\n",
    "for i in range(train_size):\n",
    "    doc_meta = shuffle_doc_name_list[i]\n",
    "    temp = doc_meta.split('\\t')\n",
    "    label = temp[2]\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    label_index = label_list.index(label)\n",
    "    one_hot[label_index] = 1\n",
    "    ally.append(one_hot)\n",
    "\n",
    "for i in range(vocab_size):\n",
    "    one_hot = [0 for l in range(len(label_list))]\n",
    "    ally.append(one_hot)\n",
    "\n",
    "ally = np.array(ally)\n",
    "\n",
    "print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word co-occurence with context windows\n",
    "window_size = 20\n",
    "windows = []\n",
    "\n",
    "for doc_words in shuffle_doc_words_list:\n",
    "    words = doc_words.split()\n",
    "    length = len(words)\n",
    "    if length <= window_size:\n",
    "        windows.append(words)\n",
    "    else:\n",
    "        # print(length, length - window_size + 1)\n",
    "        for j in range(length - window_size + 1):\n",
    "            window = words[j: j + window_size]\n",
    "            windows.append(window)\n",
    "            # print(window)\n",
    "\n",
    "\n",
    "word_window_freq = {}\n",
    "for window in windows:\n",
    "    appeared = set()\n",
    "    for i in range(len(window)):\n",
    "        if window[i] in appeared:\n",
    "            continue\n",
    "        if window[i] in word_window_freq:\n",
    "            word_window_freq[window[i]] += 1\n",
    "        else:\n",
    "            word_window_freq[window[i]] = 1\n",
    "        appeared.add(window[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_pair_count = {}\n",
    "for window in windows:\n",
    "    for i in range(1, len(window)):\n",
    "        for j in range(0, i):\n",
    "            word_i = window[i]\n",
    "            word_i_id = word_id_map[word_i]\n",
    "            word_j = window[j]\n",
    "            word_j_id = word_id_map[word_j]\n",
    "            if word_i_id == word_j_id:\n",
    "                continue\n",
    "            word_pair_str = str(word_i_id) + ',' + str(word_j_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n",
    "            # two orders\n",
    "            word_pair_str = str(word_j_id) + ',' + str(word_i_id)\n",
    "            if word_pair_str in word_pair_count:\n",
    "                word_pair_count[word_pair_str] += 1\n",
    "            else:\n",
    "                word_pair_count[word_pair_str] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row = []\n",
    "col = []\n",
    "weight = []\n",
    "\n",
    "# pmi as weights\n",
    "\n",
    "num_window = len(windows)\n",
    "\n",
    "for key in word_pair_count:\n",
    "    temp = key.split(',')\n",
    "    i = int(temp[0])\n",
    "    j = int(temp[1])\n",
    "    count = word_pair_count[key]\n",
    "    word_freq_i = word_window_freq[vocab[i]]\n",
    "    word_freq_j = word_window_freq[vocab[j]]\n",
    "    pmi = log((1.0 * count / num_window) /\n",
    "              (1.0 * word_freq_i * word_freq_j/(num_window * num_window)))\n",
    "    if pmi <= 0:\n",
    "        continue\n",
    "    row.append(train_size + i)\n",
    "    col.append(train_size + j)\n",
    "    weight.append(pmi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc word frequency\n",
    "doc_word_freq = {}\n",
    "\n",
    "for doc_id in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[doc_id]\n",
    "    words = doc_words.split()\n",
    "    for word in words:\n",
    "        word_id = word_id_map[word]\n",
    "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
    "        if doc_word_str in doc_word_freq:\n",
    "            doc_word_freq[doc_word_str] += 1\n",
    "        else:\n",
    "            doc_word_freq[doc_word_str] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(shuffle_doc_words_list)):\n",
    "    doc_words = shuffle_doc_words_list[i]\n",
    "    words = doc_words.split()\n",
    "    doc_word_set = set()\n",
    "    for word in words:\n",
    "        if word in doc_word_set:\n",
    "            continue\n",
    "        j = word_id_map[word]\n",
    "        key = str(i) + ',' + str(j)\n",
    "        freq = doc_word_freq[key]\n",
    "        if i < train_size:\n",
    "            row.append(i)\n",
    "        else:\n",
    "            row.append(i + vocab_size)\n",
    "        col.append(train_size + j)\n",
    "        idf = log(1.0 * len(shuffle_doc_words_list) /\n",
    "                  word_doc_freq[vocab[j]])\n",
    "        weight.append(freq * idf)\n",
    "        doc_word_set.add(word)\n",
    "\n",
    "node_size = train_size + vocab_size + test_size\n",
    "adj = sp.csr_matrix(\n",
    "    (weight, (row, col)), shape=(node_size, node_size))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'adj']\n",
    "objects = []\n",
    "\n",
    "x, y, tx, ty, allx, ally, adj = tuple(objects)\n",
    "# print(x.shape, y.shape, tx.shape, ty.shape, allx.shape, ally.shape)\n",
    "\n",
    "features = sp.vstack((allx, tx)).tolil()\n",
    "labels = np.vstack((ally, ty))\n",
    "# print(len(labels))\n",
    "\n",
    "train_idx_orig = parse_index_file(\n",
    "    \"./data/{}.train.index\".format(dataset_str))\n",
    "train_size = len(train_idx_orig)\n",
    "\n",
    "val_size = train_size - x.shape[0]\n",
    "test_size = tx.shape[0]\n",
    "\n",
    "idx_train = range(len(y))\n",
    "idx_val = range(len(y), len(y) + val_size)\n",
    "idx_test = range(allx.shape[0], allx.shape[0] + test_size)\n",
    "\n",
    "\n",
    "\n",
    "train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "y_train = np.zeros(labels.shape)\n",
    "y_val = np.zeros(labels.shape)\n",
    "y_test = np.zeros(labels.shape)\n",
    "y_train[train_mask, :] = labels[train_mask, :]\n",
    "y_val[val_mask, :] = labels[val_mask, :]\n",
    "y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
    "\n",
    "features = sp.identity(features.shape[0])  # featureless\n",
    "\n",
    "\n",
    "# Define placeholders\n",
    "t_features = torch.from_numpy(features)\n",
    "t_y_train = torch.from_numpy(y_train)\n",
    "t_y_val = torch.from_numpy(y_val)\n",
    "t_y_test = torch.from_numpy(y_test)\n",
    "t_train_mask = torch.from_numpy(train_mask.astype(np.float32))\n",
    "tm_train_mask = torch.transpose(torch.unsqueeze(t_train_mask, 0), 1, 0).repeat(1, y_train.shape[1])\n",
    "\n",
    "t_support = []\n",
    "for i in range(len(support)):\n",
    "    t_support.append(torch.Tensor(support[i]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class GraphConvolution(nn.Module):\n",
    "    def __init__( self, input_dim, \\\n",
    "                        output_dim, \\\n",
    "                        support, \\\n",
    "                        act_func = None, \\\n",
    "                        featureless = False, \\\n",
    "                        dropout_rate = 0., \\\n",
    "                        bias=False):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.support = support\n",
    "        self.featureless = featureless\n",
    "\n",
    "        for i in range(len(self.support)):\n",
    "            setattr(self, 'W{}'.format(i), nn.Parameter(torch.randn(input_dim, output_dim)))\n",
    "\n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(1, output_dim))\n",
    "\n",
    "        self.act_func = act_func\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        for i in range(len(self.support)):\n",
    "            if self.featureless:\n",
    "                pre_sup = getattr(self, 'W{}'.format(i))\n",
    "            else:\n",
    "                pre_sup = x.mm(getattr(self, 'W{}'.format(i)))\n",
    "            \n",
    "            if i == 0:\n",
    "                out = self.support[i].mm(pre_sup)\n",
    "            else:\n",
    "                out += self.support[i].mm(pre_sup)\n",
    "\n",
    "        if self.act_func is not None:\n",
    "            out = self.act_func(out)\n",
    "\n",
    "        self.embedding = out\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__( self, input_dim, \\\n",
    "                        support,\\\n",
    "                        dropout_rate=0., \\\n",
    "                        num_classes=10):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        # GraphConvolution\n",
    "        self.layer1 = GraphConvolution(input_dim, 200, support, act_func=nn.ReLU(), featureless=True, dropout_rate=dropout_rate)\n",
    "        self.layer2 = GraphConvolution(200, num_classes, support, dropout_rate=dropout_rate)\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(input_dim=features.shape[0], support=t_support, num_classes=y_train.shape[1])\n",
    "\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=cfg.learning_rate)\n",
    "\n",
    "\n",
    "# Define model evaluation function\n",
    "def evaluate(features, labels, mask):\n",
    "    t_test = time.time()\n",
    "    # feed_dict_val = construct_feed_dict(\n",
    "    #     features, support, labels, mask, placeholders)\n",
    "    # outs_val = sess.run([model.loss, model.accuracy, model.pred, model.labels], feed_dict=feed_dict_val)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(features)\n",
    "        t_mask = torch.from_numpy(np.array(mask*1., dtype=np.float32))\n",
    "        tm_mask = torch.transpose(torch.unsqueeze(t_mask, 0), 1, 0).repeat(1, labels.shape[1])\n",
    "        loss = criterion(logits * tm_mask, torch.max(labels, 1)[1])\n",
    "        pred = torch.max(logits, 1)[1]\n",
    "        acc = ((pred == torch.max(labels, 1)[1]).float() * t_mask).sum().item() / t_mask.sum().item()\n",
    "        \n",
    "    return loss.numpy(), acc, pred.numpy(), labels.numpy(), (time.time() - t_test)\n",
    "\n",
    "\n",
    "\n",
    "val_losses = []\n",
    "\n",
    "# Train model\n",
    "for epoch in range(cfg.epochs):\n",
    "\n",
    "    t = time.time()\n",
    "    \n",
    "    # Forward pass\n",
    "    logits = model(t_features)\n",
    "    loss = criterion(logits * tm_train_mask, torch.max(t_y_train, 1)[1])    \n",
    "    acc = ((torch.max(logits, 1)[1] == torch.max(t_y_train, 1)[1]).float() * t_train_mask).sum().item() / t_train_mask.sum().item()\n",
    "        \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_acc, pred, labels, duration = evaluate(t_features, t_y_val, val_mask)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print_log(\"Epoch: {:.0f}, train_loss= {:.5f}, train_acc= {:.5f}, val_loss= {:.5f}, val_acc= {:.5f}, time= {:.5f}\"\\\n",
    "                .format(epoch + 1, loss, acc, val_loss, val_acc, time.time() - t))\n",
    "\n",
    "    if epoch > cfg.early_stopping and val_losses[-1] > np.mean(val_losses[-(cfg.early_stopping+1):-1]):\n",
    "        print_log(\"Early stopping...\")\n",
    "        break\n",
    "\n",
    "\n",
    "print_log(\"Optimization Finished!\")\n",
    "\n",
    "\n",
    "# Testing\n",
    "test_loss, test_acc, pred, labels, test_duration = evaluate(t_features, t_y_test, test_mask)\n",
    "print_log(\"Test set results: \\n\\t loss= {:.5f}, accuracy= {:.5f}, time= {:.5f}\".format(test_loss, test_acc, test_duration))\n",
    "\n",
    "test_pred = []\n",
    "test_labels = []\n",
    "for i in range(len(test_mask)):\n",
    "    if test_mask[i]:\n",
    "        test_pred.append(pred[i])\n",
    "        test_labels.append(np.argmax(labels[i]))\n",
    "\n",
    "\n",
    "print_log(\"Test Precision, Recall and F1-Score...\")\n",
    "print_log(metrics.classification_report(test_labels, test_pred, digits=4))\n",
    "print_log(\"Macro average Test Precision, Recall and F1-Score...\")\n",
    "print_log(metrics.precision_recall_fscore_support(test_labels, test_pred, average='macro'))\n",
    "print_log(\"Micro average Test Precision, Recall and F1-Score...\")\n",
    "print_log(metrics.precision_recall_fscore_support(test_labels, test_pred, average='micro'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
